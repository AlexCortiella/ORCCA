{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "3033ebde",
   "metadata": {},
   "outputs": [],
   "source": [
    "#####################\n",
    "#### DATAMODULE #####\n",
    "#####################\n",
    "\n",
    "import torch\n",
    "import pytorch_lightning as pl\n",
    "from torch.utils.data import random_split, DataLoader, Dataset\n",
    "\n",
    "# Note - you must have torchvision installed for this example\n",
    "from torchvision import transforms\n",
    "\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import os\n",
    "from torchvision.io import read_image\n",
    "import torchvision\n",
    "import json\n",
    "import matplotlib.animation as animation\n",
    "from matplotlib.patches import Ellipse\n",
    "# import matplotlib.transforms as transforms\n",
    "import shutil\n",
    "\n",
    "\n",
    "## Data loader\n",
    "\n",
    "class ImageDataset(Dataset):\n",
    "    def __init__(self, data_dir, transform=None, target_transform=None):\n",
    "        img_dir = os.path.join(data_dir,'images')\n",
    "        lbl_dir = os.path.join(data_dir,'labels')\n",
    "        self.img_ID = [os.path.splitext(img)[0] for img in os.listdir(img_dir)]\n",
    "        self.img_type = [os.path.splitext(img)[1] for img in os.listdir(img_dir)]\n",
    "        self.img_dir = img_dir\n",
    "        self.lbl_dir = lbl_dir\n",
    "        self.transform = lambda x : -1 + 2*(x/torch.max(x))\n",
    "        #self.transform = torchvision.transforms.Normalize()\n",
    "        self.target_transform = target_transform\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.img_ID)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        img_path = os.path.join(self.img_dir, f'{self.img_ID[idx]}{self.img_type[idx]}')\n",
    "        image = read_image(img_path, torchvision.io.ImageReadMode.UNCHANGED).to(torch.float32)\n",
    "        \n",
    "        #lbl_path = open(os.path.join(self.lbl_dir, f'{self.img_ID[idx]}.txt'),'r')\n",
    "        #lines = lbl_path.readlines()\n",
    "        \n",
    "#         labels = torch.tensor([lbl[0] for lbl in lines])\n",
    "        labels = 0\n",
    "\n",
    "        if self.transform:\n",
    "            image = self.transform(image)\n",
    "        if self.target_transform:\n",
    "            labels = self.target_transform(labels)\n",
    "        return image, labels\n",
    "    \n",
    "class ImageDataModule():\n",
    "    def __init__(self, data_dir, img_size, batch_size = 8, cuda = False):\n",
    "        super().__init__()\n",
    "        self.data_dir = data_dir\n",
    "        self.batch_size = batch_size\n",
    "        self.img_size = img_size\n",
    "        self.transform = transforms.Compose([transforms.CenterCrop(img_size),transforms.ToTensor()])\n",
    "        self.target_transform = transforms.Compose([transforms.ToTensor()])\n",
    "        self.cuda = cuda\n",
    "\n",
    "    def setup(self, stage: str):\n",
    "\n",
    "        # Assign train/val datasets for use in dataloaders\n",
    "        if stage == \"fit\":\n",
    "            self.image_train = ImageDataset(self.data_dir + '/train/')\n",
    "            self.image_val = ImageDataset(self.data_dir + '/valid/')\n",
    "\n",
    "        if stage == \"test\":\n",
    "            self.image_test = ImageDataset(self.data_dir + '/test/')\n",
    "            \n",
    "        if stage == \"outliers\":\n",
    "            self.image_outliers = ImageDataset(self.data_dir + '/outliers/')\n",
    "    def train_dataloader(self):\n",
    "        \n",
    "        if self.cuda:\n",
    "            sampler = DistributedSampler(self.image_train)\n",
    "        else:\n",
    "            sampler = None\n",
    "            \n",
    "        return DataLoader(self.image_train,\n",
    "                          batch_size=self.batch_size,\n",
    "                          pin_memory=True,\n",
    "                          shuffle=False,\n",
    "                          sampler = sampler)\n",
    "         \n",
    "    def valid_dataloader(self):\n",
    "        \n",
    "        \n",
    "        if self.cuda:\n",
    "            sampler = DistributedSampler(self.image_val)\n",
    "        else:\n",
    "            sampler = None\n",
    "            \n",
    "        return DataLoader(self.image_val,\n",
    "                          batch_size=self.batch_size,\n",
    "                          pin_memory=True,\n",
    "                          shuffle=False,\n",
    "                          sampler=sampler)\n",
    "    \n",
    "\n",
    "    def test_dataloader(self):\n",
    "        return DataLoader(self.image_test, batch_size=self.batch_size)\n",
    "    \n",
    "    def outliers_dataloader(self):\n",
    "        return DataLoader(self.image_outliers, batch_size=self.batch_size)\n",
    "    \n",
    "    \n",
    "## TEST \n",
    "\n",
    "data_dir = './data/mars_rover'\n",
    "datamodule = ImageDataModule(data_dir)\n",
    "\n",
    "datamodule.setup('fit')\n",
    "\n",
    "train_data = datamodule.train_dataloader()\n",
    "\n",
    "# plt.imshow(torch.permute(train_data.dataset[0][0],(1,2,0)))\n",
    "# for i,l in train_data:\n",
    "#     print(i.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "64d3074a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "\n",
    "class Encoder(nn.Module):\n",
    "\n",
    "    def __init__(self, encoded_space_dim=2, fc2_input_dim=128, num_channels=[8, 16, 32], in_chan = 3):\n",
    "        super().__init__()\n",
    "        self.encoded_space_dim = encoded_space_dim\n",
    "        self.num_channels = num_channels\n",
    "\n",
    "        ### Convolutional section\n",
    "        self.compress_layer = nn.Conv2d(3, 3, 3, stride=2, padding=0)\n",
    "\n",
    "        self.encoder_cnn = nn.Sequential(\n",
    "            nn.Conv2d(3, self.num_channels[0], 3, stride=2, padding=0),\n",
    "            nn.BatchNorm2d(self.num_channels[0]),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(self.num_channels[0], self.num_channels[1], 3, stride=2, padding=0),\n",
    "            nn.BatchNorm2d(self.num_channels[1]),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(self.num_channels[1], self.num_channels[2], 3, stride=2, padding=0),\n",
    "            nn.ReLU()\n",
    "        )\n",
    "\n",
    "        ### Flatten layer\n",
    "        self.flatten = nn.Flatten(start_dim=1)\n",
    "        ### Linear section\n",
    "        self.encoder_lin = nn.Sequential(\n",
    "            nn.Linear(9 * 9 * self.num_channels[2], fc2_input_dim),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(fc2_input_dim, encoded_space_dim * 2)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        for _ in range(3):\n",
    "            x = self.compress_layer(x)\n",
    "        x = self.encoder_cnn(x)\n",
    "        x = self.flatten(x)\n",
    "        x = self.encoder_lin(x)\n",
    "        mu, logsigmasq = x[:, :self.encoded_space_dim], x[:, self.encoded_space_dim:]\n",
    "        return mu, logsigmasq\n",
    "\n",
    "\n",
    "class Decoder(nn.Module):\n",
    "\n",
    "    def __init__(self, encoded_space_dim=2, fc2_input_dim=128, num_channels=[32, 16, 8], in_chan = 3):\n",
    "        super().__init__()\n",
    "        self.decoder_lin = nn.Sequential(\n",
    "            nn.Linear(encoded_space_dim, fc2_input_dim),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(fc2_input_dim, 9 * 9 * num_channels[0]),\n",
    "            nn.ReLU()\n",
    "        )\n",
    "\n",
    "        self.unflatten = nn.Unflatten(dim=1,\n",
    "                                      unflattened_size=(num_channels[0], 9, 9))\n",
    "\n",
    "        self.decoder_conv = nn.Sequential(\n",
    "            nn.ConvTranspose2d(num_channels[0], num_channels[1], 3,\n",
    "                               stride=2, output_padding=(0, 0)),\n",
    "            nn.BatchNorm2d(num_channels[1]),\n",
    "            nn.ReLU(),\n",
    "            nn.ConvTranspose2d(num_channels[1], num_channels[2], 3, stride=2,\n",
    "                               padding=0, output_padding=(0, 0)),\n",
    "            nn.BatchNorm2d(num_channels[2]),\n",
    "            nn.ReLU(),\n",
    "            nn.ConvTranspose2d(num_channels[2], 2*in_chan, 3, stride=2,\n",
    "                               padding=0, output_padding=(0, 0))\n",
    "        )\n",
    "\n",
    "\n",
    "        self.decompress_layer_0 = nn.ConvTranspose2d(2*in_chan, 2*in_chan, 3, stride=2, padding=0, output_padding=(0,0))\n",
    "        self.decompress_layer_1 = nn.ConvTranspose2d(2*in_chan, 2*in_chan, 3, stride=2, padding=0, output_padding=(1,1))\n",
    "\n",
    "    def forward(self, x):\n",
    "\n",
    "        x = self.decoder_lin(x)\n",
    "        x = self.unflatten(x)\n",
    "        x = self.decoder_conv(x)\n",
    "#         x = self.decompress_layer_0(x)\n",
    "        for _ in range(2):\n",
    "            x = self.decompress_layer_0(x)\n",
    "        x = self.decompress_layer_1(x)\n",
    "        mu, logsigmasq = x[:, :3, :, :], x[:, 3:, :, :]\n",
    "        return mu, logsigmasq"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "e7f89119",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torch.distributions import Normal\n",
    "from torch.utils.data import DataLoader\n",
    "from torchvision import datasets\n",
    "import torchvision.transforms as transforms\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import os\n",
    "\n",
    "\n",
    "r = 202211010\n",
    "np.random.seed(r)\n",
    "torch.manual_seed(r)\n",
    "\n",
    "D = 1 # number of modalities\n",
    "dim = 64 # dimension of each modality (assume to be same)\n",
    "K = 2\n",
    "Z = 4\n",
    "latent_dim = Z\n",
    "\n",
    "lr = 1e-3\n",
    "num_epochs = 1500\n",
    "batch_size = 8\n",
    "num_workers = 8\n",
    "em_reg = 1e-6\n",
    "logsigmasq_reg = em_reg\n",
    "\n",
    "means_hist = []\n",
    "mu_c_hist = []\n",
    "logsigmasq_c_hist = []\n",
    "gamma_c_train_hist = []\n",
    "gamma_c_val_hist = []\n",
    "\n",
    "\n",
    "w_rec = 1.0\n",
    "w_reg = 1.0\n",
    "w_entr = 1.0\n",
    "\n",
    "\n",
    "sim_name = F'gmvae_ld{latent_dim}_nc{K}_rec{w_rec}_reg{w_reg}_entr{w_entr}'\n",
    "data_dir = './data/mars_rover'\n",
    "\n",
    "my_datamodule = ImageDataModule(data_dir)\n",
    "my_datamodule.setup('fit')\n",
    "\n",
    "save_every = 1\n",
    "\n",
    "# N_train, W_img, H_img = train_dataset.data.shape  # 60000, 28, 28\n",
    "# N_test, _, _ = test_dataset.data.shape  # 10000, 28, 28\n",
    "\n",
    "train_loader = my_datamodule.train_dataloader()\n",
    "valid_loader = my_datamodule.valid_dataloader()\n",
    "\n",
    "train_labels = torch.Tensor([int(train_loader.dataset[i][1]) for i in range(len(train_loader.dataset))])\n",
    "valid_labels = torch.Tensor([int(valid_loader.dataset[i][1]) for i in range(len(valid_loader.dataset))])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "6dfa33c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torch.distributions import Normal\n",
    "from torch.utils.data import DataLoader\n",
    "from torchvision import datasets\n",
    "import torchvision.transforms as transforms\n",
    "# from kmeans_pytorch import kmeans\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "def encoder_step(x, encoder, decoder):\n",
    "    \"\"\" Computes a stochastic estimate of the rescaled evidence lower bound\n",
    "\n",
    "    Args:\n",
    "        x_list: length-D list of (N, data_dim) torch.tensor\n",
    "        encoder_list: length-D list of Encoder\n",
    "        decoder_list: length-D list of Decoder\n",
    "        params: dictionary of other parameters\n",
    "    Returns:\n",
    "        elbo: a (,) torch.tensor containing the estimate of the ELBO\n",
    "    \"\"\"\n",
    "    mu, logsigmasq = encoder.forward(x)\n",
    "    return mu, logsigmasq + logsigmasq_reg\n",
    "\n",
    "def em_step(z, mu, params, update_by_batch=False):\n",
    "\n",
    "    mu_c = params['mu_c'].to(device)  # (K, Z)\n",
    "    logsigmasq_c = params['logsigmasq_c'].to(device)  # (K, Z)\n",
    "    sigma_c = torch.exp(0.5 * logsigmasq_c)\n",
    "    pi_c = params['pi_c'].to(device)\n",
    "\n",
    "    log_prob_zc = Normal(mu_c, sigma_c).log_prob(z.unsqueeze(dim=1)).sum(dim=2) + torch.log(pi_c)  #[N, K]\n",
    "    log_prob_zc -= log_prob_zc.logsumexp(dim=1, keepdims=True)\n",
    "    gamma_c = torch.exp(log_prob_zc) + em_reg\n",
    "\n",
    "    denominator = torch.sum(gamma_c, dim=0).unsqueeze(1)\n",
    "    mu_c = torch.einsum('nc,nz->cz', gamma_c, mu) / denominator\n",
    "    logsigmasq_c = torch.log(torch.einsum('nc,ncz->cz', gamma_c, (mu.unsqueeze(dim=1) - mu_c) ** 2)) - torch.log(denominator)\n",
    "\n",
    "    if not update_by_batch:\n",
    "        return gamma_c, mu_c, logsigmasq_c\n",
    "\n",
    "    else:\n",
    "        hist_weights = params['hist_weights'].to(device)\n",
    "        hist_mu_c = params['hist_mu_c'].to(device)\n",
    "        hist_logsigmasq_c = params['hist_logsigmasq_c'].to(device)\n",
    "\n",
    "        curr_weights = denominator\n",
    "        new_weights = hist_weights + curr_weights\n",
    "        new_mu_c = (hist_weights * hist_mu_c + curr_weights * mu_c) / new_weights\n",
    "        new_logsigmasq_c = torch.log(hist_weights * torch.exp(hist_logsigmasq_c) + curr_weights * torch.exp(logsigmasq_c)) - torch.log(new_weights)\n",
    "        # new_logsigmasq_c = torch.log(torch.exp(torch.log(hist_weights) + hist_logsigmasq_c) +\n",
    "        #                              torch.exp(torch.log(curr_weights) + logsigmasq_c)) - torch.log(new_weights)\n",
    "\n",
    "        params['hist_weights'] = new_weights\n",
    "        params['hist_mu_c'] = new_mu_c\n",
    "        params['hist_logsigmasq_c'] = new_logsigmasq_c\n",
    "        return gamma_c, new_mu_c, new_logsigmasq_c\n",
    "\n",
    "\n",
    "def decoder_step(x, z, encoder, decoder, params, mu, logsigmasq, gamma_c):\n",
    "    \"\"\"\n",
    "    Computes a stochastic estimate of the ELBO.\n",
    "    :param x_list: length-D list of (N, data_dim) torch.tensor\n",
    "    :param z: MC samples of the encoded distributions\n",
    "    :param encoder_list: length-D list of Encoder\n",
    "    :param decoder_list: length-D list of Decoder\n",
    "    :param params: dictionary of non-DNN parameters\n",
    "    :return:\n",
    "        elbo: (,) tensor containing the elbo estimation\n",
    "    \"\"\"\n",
    "    sigma = torch.exp(0.5 * logsigmasq)\n",
    "    mu_c = params['mu_c']\n",
    "    logsigmasq_c = params['logsigmasq_c']\n",
    "    pi_c = params['pi_c']\n",
    "\n",
    "    elbo = 0\n",
    "    \n",
    "    reconstruction = 0\n",
    "    regularization = 0\n",
    "    entropy = 0\n",
    "    \n",
    "    mu_, logsigmasq_ = decoder.forward(z)\n",
    "    reconstruction += Normal(mu_, torch.exp(0.5 * logsigmasq_)).log_prob(x).sum()\n",
    "        \n",
    "    regularization = - 0.5 * torch.sum(gamma_c * (logsigmasq_c + (sigma.unsqueeze(1) ** 2 + (mu.unsqueeze(1) - mu_c) ** 2) /\n",
    "                                         torch.exp(logsigmasq_c)).sum(dim=2))\n",
    "    \n",
    "    entropy = torch.sum(gamma_c * (torch.log(pi_c) - torch.log(gamma_c))) + 0.5 * torch.sum(1 + logsigmasq)\n",
    "\n",
    "    elbo = w_rec*reconstruction + w_reg*regularization + w_entr*entropy\n",
    "    \n",
    "    return elbo, reconstruction, regularization, entropy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1871b934",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "====> Epoch: 0 Train ELBO: -1181861.5126 Val ELBO: -1005192.7614\n",
      "====> Epoch: 1 Train ELBO: -634175.4725 Val ELBO: -105426.8876\n",
      "====> Epoch: 2 Train ELBO: -25561.3768 Val ELBO: 84014.1024\n",
      "====> Epoch: 3 Train ELBO: 131526.6517 Val ELBO: 241228.0934\n",
      "====> Epoch: 4 Train ELBO: 187156.7730 Val ELBO: 288028.7244\n",
      "====> Epoch: 5 Train ELBO: 205100.3277 Val ELBO: -42959.6819\n",
      "====> Epoch: 6 Train ELBO: 208730.5198 Val ELBO: 219280.6204\n",
      "====> Epoch: 7 Train ELBO: 222541.6588 Val ELBO: 351017.7997\n",
      "====> Epoch: 8 Train ELBO: 240508.8355 Val ELBO: 350586.7670\n",
      "====> Epoch: 9 Train ELBO: 254153.9512 Val ELBO: 342937.8622\n",
      "====> Epoch: 10 Train ELBO: 265093.9903 Val ELBO: 357411.9673\n",
      "====> Epoch: 11 Train ELBO: 266537.5737 Val ELBO: 334134.4659\n",
      "====> Epoch: 12 Train ELBO: 268428.7328 Val ELBO: 309306.0909\n",
      "====> Epoch: 13 Train ELBO: 280760.4525 Val ELBO: 354236.9716\n",
      "====> Epoch: 14 Train ELBO: 291501.8950 Val ELBO: 384733.8310\n",
      "====> Epoch: 15 Train ELBO: 303206.3602 Val ELBO: 374949.2912\n",
      "====> Epoch: 16 Train ELBO: 312682.7556 Val ELBO: 409339.3580\n",
      "====> Epoch: 17 Train ELBO: 322715.5002 Val ELBO: 404248.0668\n",
      "====> Epoch: 18 Train ELBO: 327197.3036 Val ELBO: 418492.3125\n",
      "====> Epoch: 19 Train ELBO: 341318.9279 Val ELBO: 423525.0114\n",
      "====> Epoch: 20 Train ELBO: 348695.3167 Val ELBO: 406558.0256\n",
      "====> Epoch: 21 Train ELBO: 362821.4708 Val ELBO: 421800.8352\n",
      "====> Epoch: 22 Train ELBO: 368466.9326 Val ELBO: 461726.5341\n",
      "====> Epoch: 23 Train ELBO: 373369.7544 Val ELBO: 421006.5795\n",
      "====> Epoch: 24 Train ELBO: 386899.1166 Val ELBO: 469512.4943\n",
      "====> Epoch: 25 Train ELBO: 400106.0678 Val ELBO: 478136.3523\n",
      "====> Epoch: 26 Train ELBO: 407007.0429 Val ELBO: 471941.1562\n",
      "====> Epoch: 27 Train ELBO: 415279.4774 Val ELBO: 459995.7273\n",
      "====> Epoch: 28 Train ELBO: 421372.9830 Val ELBO: 246171.2237\n",
      "====> Epoch: 29 Train ELBO: 417856.3895 Val ELBO: 483400.2358\n",
      "====> Epoch: 30 Train ELBO: 425977.9289 Val ELBO: 501490.2216\n",
      "====> Epoch: 31 Train ELBO: 441085.2035 Val ELBO: 513584.5881\n",
      "====> Epoch: 32 Train ELBO: 453505.9158 Val ELBO: 443681.7102\n",
      "====> Epoch: 33 Train ELBO: 461462.4231 Val ELBO: 480678.2812\n",
      "====> Epoch: 34 Train ELBO: 464735.0898 Val ELBO: 527703.3068\n",
      "====> Epoch: 35 Train ELBO: 471003.9294 Val ELBO: 535833.1392\n",
      "====> Epoch: 36 Train ELBO: 478745.2995 Val ELBO: 507343.7301\n",
      "====> Epoch: 37 Train ELBO: 487452.5219 Val ELBO: 525316.7898\n",
      "====> Epoch: 38 Train ELBO: 487260.0204 Val ELBO: 444326.3011\n",
      "====> Epoch: 39 Train ELBO: 467513.2231 Val ELBO: 500499.6165\n",
      "====> Epoch: 40 Train ELBO: 470030.0843 Val ELBO: 337493.2330\n",
      "====> Epoch: 41 Train ELBO: 482187.6075 Val ELBO: 316136.1080\n",
      "====> Epoch: 42 Train ELBO: 490162.2210 Val ELBO: 488180.1222\n",
      "====> Epoch: 43 Train ELBO: 491750.1706 Val ELBO: 474911.6761\n",
      "====> Epoch: 44 Train ELBO: 489731.0680 Val ELBO: 533138.3835\n",
      "====> Epoch: 45 Train ELBO: 479359.6335 Val ELBO: 506085.4602\n",
      "====> Epoch: 46 Train ELBO: 491075.0414 Val ELBO: 451974.5710\n",
      "====> Epoch: 47 Train ELBO: 505652.1629 Val ELBO: 417521.4006\n",
      "====> Epoch: 48 Train ELBO: 514564.5587 Val ELBO: 529643.1420\n",
      "====> Epoch: 49 Train ELBO: 507130.6466 Val ELBO: 496915.2486\n",
      "====> Epoch: 50 Train ELBO: 498356.7118 Val ELBO: 529140.1335\n",
      "====> Epoch: 51 Train ELBO: 487993.7595 Val ELBO: 485399.2585\n",
      "====> Epoch: 52 Train ELBO: 460751.5371 Val ELBO: 431812.9006\n",
      "====> Epoch: 53 Train ELBO: 399848.2779 Val ELBO: 535484.8523\n",
      "====> Epoch: 54 Train ELBO: 489231.7667 Val ELBO: 557321.2926\n",
      "====> Epoch: 55 Train ELBO: 516019.8040 Val ELBO: 535697.2585\n",
      "====> Epoch: 56 Train ELBO: 536201.8554 Val ELBO: 551713.4773\n",
      "====> Epoch: 57 Train ELBO: 545305.8281 Val ELBO: 568821.3153\n",
      "====> Epoch: 58 Train ELBO: 549682.7516 Val ELBO: 581224.4545\n",
      "====> Epoch: 59 Train ELBO: 553292.9653 Val ELBO: 588208.7926\n",
      "====> Epoch: 60 Train ELBO: 553264.0593 Val ELBO: 573181.4631\n",
      "====> Epoch: 61 Train ELBO: 531381.7323 Val ELBO: 526816.6193\n",
      "====> Epoch: 62 Train ELBO: 482992.7743 Val ELBO: 103397.4176\n",
      "====> Epoch: 63 Train ELBO: 507800.3895 Val ELBO: 413277.4602\n",
      "====> Epoch: 64 Train ELBO: 518196.8737 Val ELBO: 570334.9460\n",
      "====> Epoch: 65 Train ELBO: 565148.3085 Val ELBO: 591713.5824\n",
      "====> Epoch: 66 Train ELBO: 600079.4765 Val ELBO: 608566.4233\n",
      "====> Epoch: 67 Train ELBO: 615756.4242 Val ELBO: 554251.9148\n",
      "====> Epoch: 68 Train ELBO: 621370.5581 Val ELBO: 486357.8097\n",
      "====> Epoch: 69 Train ELBO: 610629.4012 Val ELBO: 579763.5653\n",
      "====> Epoch: 70 Train ELBO: 571114.7431 Val ELBO: 575024.1790\n",
      "====> Epoch: 71 Train ELBO: 569795.3188 Val ELBO: 576586.6989\n",
      "====> Epoch: 72 Train ELBO: 570258.4713 Val ELBO: 494183.4631\n",
      "====> Epoch: 73 Train ELBO: 581593.4056 Val ELBO: 392077.5014\n",
      "====> Epoch: 74 Train ELBO: 463606.1607 Val ELBO: 448846.1591\n",
      "====> Epoch: 75 Train ELBO: 356069.0093 Val ELBO: 499996.5994\n",
      "====> Epoch: 76 Train ELBO: 547474.3504 Val ELBO: 640616.1932\n",
      "====> Epoch: 77 Train ELBO: 641949.1357 Val ELBO: 642050.9233\n",
      "====> Epoch: 78 Train ELBO: 671343.3750 Val ELBO: 629986.7557\n",
      "====> Epoch: 79 Train ELBO: 687000.8264 Val ELBO: 631628.4744\n",
      "====> Epoch: 80 Train ELBO: 693677.1452 Val ELBO: 625033.7528\n",
      "====> Epoch: 81 Train ELBO: 689894.2929 Val ELBO: 549314.9034\n",
      "====> Epoch: 82 Train ELBO: 677482.4160 Val ELBO: 590404.1364\n",
      "====> Epoch: 83 Train ELBO: 655968.4571 Val ELBO: 542339.9261\n",
      "====> Epoch: 84 Train ELBO: 599458.8756 Val ELBO: 551807.2585\n",
      "====> Epoch: 85 Train ELBO: 537506.7246 Val ELBO: 145102.5036\n",
      "====> Epoch: 86 Train ELBO: 481286.0271 Val ELBO: 527016.6562\n",
      "====> Epoch: 87 Train ELBO: 614391.1252 Val ELBO: 616689.0312\n",
      "====> Epoch: 88 Train ELBO: 675573.7647 Val ELBO: 617995.5000\n",
      "====> Epoch: 89 Train ELBO: 700653.1072 Val ELBO: 642809.7642\n",
      "====> Epoch: 90 Train ELBO: 706625.4290 Val ELBO: 616361.1307\n",
      "====> Epoch: 91 Train ELBO: 678316.7986 Val ELBO: 440005.7386\n",
      "====> Epoch: 92 Train ELBO: 551713.0189 Val ELBO: 545921.1989\n",
      "====> Epoch: 93 Train ELBO: 561520.4438 Val ELBO: 609893.5398\n",
      "====> Epoch: 94 Train ELBO: 673738.3378 Val ELBO: 647704.2386\n",
      "====> Epoch: 95 Train ELBO: 690984.3100 Val ELBO: 458648.4929\n",
      "====> Epoch: 96 Train ELBO: 638053.2164 Val ELBO: 57132.9628\n",
      "====> Epoch: 97 Train ELBO: 606102.1550 Val ELBO: 594782.7273\n",
      "====> Epoch: 98 Train ELBO: 676913.9722 Val ELBO: 649514.3665\n",
      "====> Epoch: 99 Train ELBO: 712883.8441 Val ELBO: 570905.4659\n",
      "====> Epoch: 100 Train ELBO: 695688.8737 Val ELBO: 540086.0653\n",
      "====> Epoch: 101 Train ELBO: 626140.7109 Val ELBO: 523185.4858\n",
      "====> Epoch: 102 Train ELBO: 670834.1098 Val ELBO: 482261.9773\n",
      "====> Epoch: 103 Train ELBO: 648415.6209 Val ELBO: 473317.8494\n",
      "====> Epoch: 104 Train ELBO: 648011.6020 Val ELBO: 665655.8608\n",
      "====> Epoch: 105 Train ELBO: 718039.2284 Val ELBO: 611339.7045\n",
      "====> Epoch: 106 Train ELBO: 736454.6010 Val ELBO: 580677.9403\n",
      "====> Epoch: 107 Train ELBO: 689954.9463 Val ELBO: 528719.2585\n",
      "====> Epoch: 108 Train ELBO: 679263.2279 Val ELBO: 191878.1683\n",
      "====> Epoch: 109 Train ELBO: 659782.2765 Val ELBO: 645959.9574\n",
      "====> Epoch: 110 Train ELBO: 635717.8666 Val ELBO: 620322.8409\n",
      "====> Epoch: 111 Train ELBO: 695804.5101 Val ELBO: 589919.8636\n",
      "====> Epoch: 112 Train ELBO: 716152.9722 Val ELBO: 546799.2216\n",
      "====> Epoch: 113 Train ELBO: 698811.8617 Val ELBO: 259352.0036\n",
      "====> Epoch: 114 Train ELBO: 695758.3520 Val ELBO: 647108.1932\n",
      "====> Epoch: 115 Train ELBO: 696656.4255 Val ELBO: 598889.8438\n",
      "====> Epoch: 116 Train ELBO: 659409.6686 Val ELBO: 596086.1903\n",
      "====> Epoch: 117 Train ELBO: 702246.0455 Val ELBO: 497542.9545\n",
      "====> Epoch: 118 Train ELBO: 658121.6420 Val ELBO: 378287.3338\n",
      "====> Epoch: 119 Train ELBO: 698159.6360 Val ELBO: 591398.9432\n",
      "====> Epoch: 120 Train ELBO: 659306.9306 Val ELBO: 633532.5909\n",
      "====> Epoch: 121 Train ELBO: 767493.7178 Val ELBO: 602672.9062\n",
      "====> Epoch: 122 Train ELBO: 728494.7273 Val ELBO: 331743.0369\n",
      "====> Epoch: 123 Train ELBO: 719747.7266 Val ELBO: 489392.6378\n",
      "====> Epoch: 124 Train ELBO: 755892.2238 Val ELBO: 295681.3530\n",
      "====> Epoch: 125 Train ELBO: 677249.1383 Val ELBO: 654896.3381\n",
      "====> Epoch: 126 Train ELBO: 788567.7052 Val ELBO: 595886.9801\n",
      "====> Epoch: 127 Train ELBO: 765113.9646 Val ELBO: 433630.9432\n",
      "====> Epoch: 128 Train ELBO: 730477.8321 Val ELBO: 514887.1207\n",
      "====> Epoch: 129 Train ELBO: 783212.6452 Val ELBO: 51072.6388\n",
      "====> Epoch: 130 Train ELBO: 703449.2184 Val ELBO: 659841.1591\n",
      "====> Epoch: 131 Train ELBO: 791573.1572 Val ELBO: 606338.8352\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "====> Epoch: 132 Train ELBO: 777512.2431 Val ELBO: 512100.7585\n",
      "====> Epoch: 133 Train ELBO: 747579.0925 Val ELBO: 570009.2585\n",
      "====> Epoch: 134 Train ELBO: 777518.2923 Val ELBO: 200162.6119\n",
      "====> Epoch: 135 Train ELBO: 701038.1370 Val ELBO: 633717.4915\n",
      "====> Epoch: 136 Train ELBO: 808043.5183 Val ELBO: 501792.4432\n",
      "====> Epoch: 137 Train ELBO: 782350.2229 Val ELBO: 629133.1676\n",
      "====> Epoch: 138 Train ELBO: 793050.2872 Val ELBO: -92575.3058\n",
      "====> Epoch: 139 Train ELBO: 712933.2260 Val ELBO: 607975.3722\n",
      "====> Epoch: 140 Train ELBO: 787024.8176 Val ELBO: 526641.3097\n",
      "====> Epoch: 141 Train ELBO: 787452.7942 Val ELBO: 592942.3125\n",
      "====> Epoch: 142 Train ELBO: 795463.8378 Val ELBO: 634951.5994\n",
      "====> Epoch: 143 Train ELBO: 750170.0095 Val ELBO: 465170.5227\n",
      "====> Epoch: 144 Train ELBO: 742987.6768 Val ELBO: 503914.3097\n",
      "====> Epoch: 145 Train ELBO: 758882.0063 Val ELBO: 534015.9375\n",
      "====> Epoch: 146 Train ELBO: 809380.1187 Val ELBO: 603033.1477\n",
      "====> Epoch: 147 Train ELBO: 805895.0101 Val ELBO: 544145.4489\n",
      "====> Epoch: 148 Train ELBO: 780374.7513 Val ELBO: 316693.9744\n",
      "====> Epoch: 149 Train ELBO: 746990.7932 Val ELBO: 465844.1534\n",
      "====> Epoch: 150 Train ELBO: 749977.6105 Val ELBO: 636515.1847\n",
      "====> Epoch: 151 Train ELBO: 789559.2222 Val ELBO: 647899.9489\n",
      "====> Epoch: 152 Train ELBO: 787985.4621 Val ELBO: 480649.7145\n",
      "====> Epoch: 153 Train ELBO: 763114.4482 Val ELBO: 249433.7543\n",
      "====> Epoch: 154 Train ELBO: 765596.6742 Val ELBO: 566611.4318\n",
      "====> Epoch: 155 Train ELBO: 771475.2380 Val ELBO: 645328.6080\n",
      "====> Epoch: 156 Train ELBO: 737001.2961 Val ELBO: 606882.4659\n",
      "====> Epoch: 157 Train ELBO: 738994.4388 Val ELBO: -98528.2800\n",
      "====> Epoch: 158 Train ELBO: 763751.8321 Val ELBO: 570923.2784\n",
      "====> Epoch: 159 Train ELBO: 781343.0631 Val ELBO: 637690.4773\n",
      "====> Epoch: 160 Train ELBO: 750449.0063 Val ELBO: 621608.4375\n",
      "====> Epoch: 161 Train ELBO: 720236.6831 Val ELBO: -154429.1353\n",
      "====> Epoch: 162 Train ELBO: 737197.0189 Val ELBO: 581479.2273\n",
      "====> Epoch: 163 Train ELBO: 747233.3842 Val ELBO: 645117.5511\n",
      "====> Epoch: 164 Train ELBO: 701123.1250 Val ELBO: 650193.5824\n",
      "====> Epoch: 165 Train ELBO: 727497.2393 Val ELBO: 181074.1921\n",
      "====> Epoch: 166 Train ELBO: 749118.6976 Val ELBO: 506316.0540\n",
      "====> Epoch: 167 Train ELBO: 773683.3952 Val ELBO: 664521.2443\n",
      "====> Epoch: 168 Train ELBO: 781016.5423 Val ELBO: 613620.9375\n",
      "====> Epoch: 169 Train ELBO: 751405.2431 Val ELBO: 622456.5739\n",
      "====> Epoch: 170 Train ELBO: 736347.3737 Val ELBO: 316073.2557\n",
      "====> Epoch: 171 Train ELBO: 747317.0726 Val ELBO: 380228.7116\n",
      "====> Epoch: 172 Train ELBO: 765435.3311 Val ELBO: 654509.9034\n",
      "====> Epoch: 173 Train ELBO: 760690.1433 Val ELBO: 563139.0682\n",
      "====> Epoch: 174 Train ELBO: 776838.3763 Val ELBO: 586793.4347\n",
      "====> Epoch: 175 Train ELBO: 762182.5713 Val ELBO: 341563.3906\n",
      "====> Epoch: 176 Train ELBO: 769273.0922 Val ELBO: 506482.8665\n",
      "====> Epoch: 177 Train ELBO: 803662.2143 Val ELBO: 655770.5284\n",
      "====> Epoch: 178 Train ELBO: 773691.8888 Val ELBO: 513673.2401\n",
      "====> Epoch: 179 Train ELBO: 786294.4640 Val ELBO: 662268.6023\n",
      "====> Epoch: 180 Train ELBO: 774716.5265 Val ELBO: 445738.7330\n",
      "====> Epoch: 181 Train ELBO: 788808.0543 Val ELBO: 531542.8153\n",
      "====> Epoch: 182 Train ELBO: 850891.4675 Val ELBO: 633870.2983\n",
      "====> Epoch: 183 Train ELBO: 827096.3413 Val ELBO: 485641.1009\n",
      "====> Epoch: 184 Train ELBO: 795536.6503 Val ELBO: 589863.4403\n",
      "====> Epoch: 185 Train ELBO: 805229.6187 Val ELBO: 579070.0881\n",
      "====> Epoch: 186 Train ELBO: 798303.0676 Val ELBO: 543651.0824\n",
      "====> Epoch: 187 Train ELBO: 816878.0145 Val ELBO: 602515.9517\n",
      "====> Epoch: 188 Train ELBO: 849128.4323 Val ELBO: 512350.9830\n",
      "====> Epoch: 189 Train ELBO: 803769.4246 Val ELBO: 501226.7642\n",
      "====> Epoch: 190 Train ELBO: 812665.1231 Val ELBO: 657986.7244\n",
      "====> Epoch: 191 Train ELBO: 816175.6471 Val ELBO: 546876.5028\n",
      "====> Epoch: 192 Train ELBO: 803758.0196 Val ELBO: 625114.9119\n",
      "====> Epoch: 193 Train ELBO: 839964.6701 Val ELBO: 600739.9375\n",
      "====> Epoch: 194 Train ELBO: 833163.4572 Val ELBO: 302949.2667\n",
      "====> Epoch: 195 Train ELBO: 785979.9009 Val ELBO: 622792.0256\n",
      "====> Epoch: 196 Train ELBO: 824743.9798 Val ELBO: 634308.9631\n",
      "====> Epoch: 197 Train ELBO: 848948.0006 Val ELBO: 598323.2188\n",
      "====> Epoch: 198 Train ELBO: 830304.1465 Val ELBO: 635368.5540\n",
      "====> Epoch: 199 Train ELBO: 842214.6136 Val ELBO: 438607.9943\n",
      "====> Epoch: 200 Train ELBO: 815923.1749 Val ELBO: 474434.5540\n",
      "====> Epoch: 201 Train ELBO: 771614.3516 Val ELBO: 658472.5852\n",
      "====> Epoch: 202 Train ELBO: 847182.6477 Val ELBO: 617246.5398\n",
      "====> Epoch: 203 Train ELBO: 848493.2670 Val ELBO: 652162.3409\n",
      "====> Epoch: 204 Train ELBO: 844982.5224 Val ELBO: 566063.0199\n",
      "====> Epoch: 205 Train ELBO: 841626.1234 Val ELBO: 347475.3800\n",
      "====> Epoch: 206 Train ELBO: 815887.8497 Val ELBO: 653704.8864\n",
      "====> Epoch: 207 Train ELBO: 788694.7784 Val ELBO: 625338.0170\n",
      "====> Epoch: 208 Train ELBO: 828785.0802 Val ELBO: 664478.0170\n",
      "====> Epoch: 209 Train ELBO: 866132.5650 Val ELBO: 621677.9148\n",
      "====> Epoch: 210 Train ELBO: 847597.7819 Val ELBO: 495415.9972\n",
      "====> Epoch: 211 Train ELBO: 840855.4779 Val ELBO: 438158.4673\n",
      "====> Epoch: 212 Train ELBO: 837113.1117 Val ELBO: 586075.4432\n",
      "====> Epoch: 213 Train ELBO: 788987.2071 Val ELBO: 609763.7386\n",
      "====> Epoch: 214 Train ELBO: 846901.2784 Val ELBO: 655698.1449\n",
      "====> Epoch: 215 Train ELBO: 896990.0581 Val ELBO: 578889.5625\n",
      "====> Epoch: 216 Train ELBO: 878143.2260 Val ELBO: 464249.8814\n",
      "====> Epoch: 217 Train ELBO: 875398.1439 Val ELBO: 463469.2987\n",
      "====> Epoch: 218 Train ELBO: 873282.9994 Val ELBO: 448784.2912\n",
      "====> Epoch: 219 Train ELBO: 813163.3176 Val ELBO: 592175.4176\n",
      "====> Epoch: 220 Train ELBO: 858428.4678 Val ELBO: 656921.4688\n",
      "====> Epoch: 221 Train ELBO: 904375.3024 Val ELBO: 581665.9091\n",
      "====> Epoch: 222 Train ELBO: 889968.4375 Val ELBO: 465709.4524\n",
      "====> Epoch: 223 Train ELBO: 881388.9362 Val ELBO: 487772.3004\n",
      "====> Epoch: 224 Train ELBO: 885875.7449 Val ELBO: 272947.7102\n",
      "====> Epoch: 225 Train ELBO: 830962.4419 Val ELBO: 603542.8864\n",
      "====> Epoch: 226 Train ELBO: 865526.0713 Val ELBO: 629898.6307\n",
      "====> Epoch: 227 Train ELBO: 907846.0518 Val ELBO: 545232.3608\n",
      "====> Epoch: 228 Train ELBO: 889998.5606 Val ELBO: 477259.2621\n",
      "====> Epoch: 229 Train ELBO: 879096.3011 Val ELBO: 520137.9531\n",
      "====> Epoch: 230 Train ELBO: 871617.9905 Val ELBO: 415612.2571\n",
      "====> Epoch: 231 Train ELBO: 847316.5991 Val ELBO: 661283.7415\n",
      "====> Epoch: 232 Train ELBO: 898780.9533 Val ELBO: 600234.4233\n",
      "====> Epoch: 233 Train ELBO: 916881.9848 Val ELBO: 510620.2045\n",
      "====> Epoch: 234 Train ELBO: 899847.8636 Val ELBO: 501601.1009\n",
      "====> Epoch: 235 Train ELBO: 870755.1907 Val ELBO: 301960.9875\n",
      "====> Epoch: 236 Train ELBO: 863078.2633 Val ELBO: 667652.8807\n",
      "====> Epoch: 237 Train ELBO: 902764.5896 Val ELBO: 562770.9375\n",
      "====> Epoch: 238 Train ELBO: 897932.2569 Val ELBO: 565588.7244\n",
      "====> Epoch: 239 Train ELBO: 890977.6717 Val ELBO: 429881.5057\n",
      "====> Epoch: 240 Train ELBO: 861993.3340 Val ELBO: 514534.8253\n",
      "====> Epoch: 241 Train ELBO: 858025.8965 Val ELBO: 649757.7074\n",
      "====> Epoch: 242 Train ELBO: 890385.8516 Val ELBO: 565695.2699\n",
      "====> Epoch: 243 Train ELBO: 913849.7178 Val ELBO: 539374.1705\n",
      "====> Epoch: 244 Train ELBO: 864887.7292 Val ELBO: 519837.5994\n",
      "====> Epoch: 245 Train ELBO: 866649.1143 Val ELBO: 534960.7642\n",
      "====> Epoch: 246 Train ELBO: 859625.1383 Val ELBO: 648203.8523\n",
      "====> Epoch: 247 Train ELBO: 853525.1723 Val ELBO: 506456.8366\n",
      "====> Epoch: 248 Train ELBO: 907496.1244 Val ELBO: 535335.3494\n",
      "====> Epoch: 249 Train ELBO: 887245.2128 Val ELBO: 454258.5085\n",
      "====> Epoch: 250 Train ELBO: 897906.2279 Val ELBO: 458163.5185\n",
      "====> Epoch: 251 Train ELBO: 892034.1383 Val ELBO: 568460.0341\n",
      "====> Epoch: 252 Train ELBO: 882018.5379 Val ELBO: 448363.2131\n",
      "====> Epoch: 253 Train ELBO: 901912.4179 Val ELBO: 600610.8409\n",
      "====> Epoch: 254 Train ELBO: 894445.8933 Val ELBO: 443627.4375\n",
      "====> Epoch: 255 Train ELBO: 871566.2936 Val ELBO: 409011.8153\n",
      "====> Epoch: 256 Train ELBO: 885479.5189 Val ELBO: 521726.3239\n",
      "====> Epoch: 257 Train ELBO: 874589.8289 Val ELBO: 475059.9418\n",
      "====> Epoch: 258 Train ELBO: 901278.0423 Val ELBO: 582183.7415\n",
      "====> Epoch: 259 Train ELBO: 883081.2582 Val ELBO: 338172.0739\n",
      "====> Epoch: 260 Train ELBO: 874014.1768 Val ELBO: 471032.8011\n",
      "====> Epoch: 261 Train ELBO: 888104.0196 Val ELBO: 558321.2557\n",
      "====> Epoch: 262 Train ELBO: 873651.6503 Val ELBO: 505686.7372\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "====> Epoch: 263 Train ELBO: 878185.3403 Val ELBO: 461287.8920\n",
      "====> Epoch: 264 Train ELBO: 896579.9527 Val ELBO: 438005.3011\n",
      "====> Epoch: 265 Train ELBO: 907009.6641 Val ELBO: 463540.2926\n",
      "====> Epoch: 266 Train ELBO: 916825.2140 Val ELBO: 505571.1619\n",
      "====> Epoch: 267 Train ELBO: 908904.1900 Val ELBO: 559741.5511\n",
      "====> Epoch: 268 Train ELBO: 889131.3371 Val ELBO: 475276.5085\n",
      "====> Epoch: 269 Train ELBO: 890631.0694 Val ELBO: 556902.5710\n",
      "====> Epoch: 270 Train ELBO: 897002.7910 Val ELBO: 382791.6009\n",
      "====> Epoch: 271 Train ELBO: 879157.3125 Val ELBO: 515920.8537\n",
      "====> Epoch: 272 Train ELBO: 880970.0878 Val ELBO: 581939.3381\n",
      "====> Epoch: 273 Train ELBO: 902009.8889 Val ELBO: 520836.5824\n",
      "====> Epoch: 274 Train ELBO: 929652.0732 Val ELBO: 536743.1989\n",
      "====> Epoch: 275 Train ELBO: 921620.7563 Val ELBO: 439405.8260\n",
      "====> Epoch: 276 Train ELBO: 906564.1970 Val ELBO: 491599.1037\n",
      "====> Epoch: 277 Train ELBO: 865494.8428 Val ELBO: 599035.1562\n",
      "====> Epoch: 278 Train ELBO: 892081.9097 Val ELBO: 569485.0057\n",
      "====> Epoch: 279 Train ELBO: 919592.7121 Val ELBO: 562817.9290\n",
      "====> Epoch: 280 Train ELBO: 919072.5890 Val ELBO: 414300.9162\n",
      "====> Epoch: 281 Train ELBO: 906408.5631 Val ELBO: 530609.7131\n",
      "====> Epoch: 282 Train ELBO: 868019.3674 Val ELBO: 558724.5114\n",
      "====> Epoch: 283 Train ELBO: 894340.1023 Val ELBO: 587259.3466\n",
      "====> Epoch: 284 Train ELBO: 933132.7184 Val ELBO: 514392.5781\n",
      "====> Epoch: 285 Train ELBO: 925790.2708 Val ELBO: 341371.8467\n",
      "====> Epoch: 286 Train ELBO: 924133.3624 Val ELBO: 502263.2017\n",
      "====> Epoch: 287 Train ELBO: 897729.4697 Val ELBO: 551240.0881\n",
      "====> Epoch: 288 Train ELBO: 911376.0505 Val ELBO: 601591.4659\n",
      "====> Epoch: 289 Train ELBO: 936760.0082 Val ELBO: 466463.4673\n",
      "====> Epoch: 290 Train ELBO: 906738.3428 Val ELBO: 354532.3008\n",
      "====> Epoch: 291 Train ELBO: 929245.0713 Val ELBO: 469063.3494\n",
      "====> Epoch: 292 Train ELBO: 903546.1806 Val ELBO: 602881.0455\n",
      "====> Epoch: 293 Train ELBO: 935869.0808 Val ELBO: 617721.3239\n",
      "====> Epoch: 294 Train ELBO: 949173.8971 Val ELBO: 420286.7905\n",
      "====> Epoch: 295 Train ELBO: 903645.4324 Val ELBO: 405267.2429\n",
      "====> Epoch: 296 Train ELBO: 938762.0455 Val ELBO: 504765.5426\n",
      "====> Epoch: 297 Train ELBO: 923438.3100 Val ELBO: 619281.6932\n",
      "====> Epoch: 298 Train ELBO: 942508.5234 Val ELBO: 606689.2244\n",
      "====> Epoch: 299 Train ELBO: 962331.0473 Val ELBO: 419694.9162\n",
      "====> Epoch: 300 Train ELBO: 935324.8327 Val ELBO: 449353.7557\n",
      "====> Epoch: 301 Train ELBO: 946452.6951 Val ELBO: 502774.4929\n",
      "====> Epoch: 302 Train ELBO: 932177.6761 Val ELBO: 603689.0625\n",
      "====> Epoch: 303 Train ELBO: 931288.7917 Val ELBO: 595392.1705\n",
      "====> Epoch: 304 Train ELBO: 958916.5183 Val ELBO: 479167.2493\n",
      "====> Epoch: 305 Train ELBO: 921811.2418 Val ELBO: 427083.6321\n",
      "====> Epoch: 306 Train ELBO: 939639.0846 Val ELBO: 504197.8509\n",
      "====> Epoch: 307 Train ELBO: 939735.0997 Val ELBO: 592837.9801\n",
      "====> Epoch: 308 Train ELBO: 935232.4710 Val ELBO: 579327.7955\n",
      "====> Epoch: 309 Train ELBO: 969038.5322 Val ELBO: 470140.4581\n",
      "====> Epoch: 310 Train ELBO: 916899.2973 Val ELBO: 432314.3139\n",
      "====> Epoch: 311 Train ELBO: 944953.0593 Val ELBO: 526956.3580\n",
      "====> Epoch: 312 Train ELBO: 908503.6963 Val ELBO: 550273.0142\n",
      "====> Epoch: 313 Train ELBO: 939650.1774 Val ELBO: 543862.0199\n",
      "====> Epoch: 314 Train ELBO: 960426.5360 Val ELBO: 502322.5433\n",
      "====> Epoch: 315 Train ELBO: 935227.0600 Val ELBO: 459386.8977\n",
      "====> Epoch: 316 Train ELBO: 931895.7986 Val ELBO: 400088.0405\n",
      "====> Epoch: 317 Train ELBO: 932993.6907 Val ELBO: 466383.2514\n",
      "====> Epoch: 318 Train ELBO: 942682.4615 Val ELBO: 570400.1364\n",
      "====> Epoch: 319 Train ELBO: 958783.1067 Val ELBO: 540212.4574\n",
      "====> Epoch: 320 Train ELBO: 967396.8573 Val ELBO: 468904.1676\n",
      "====> Epoch: 321 Train ELBO: 950491.3258 Val ELBO: 457282.1648\n",
      "====> Epoch: 322 Train ELBO: 958412.0316 Val ELBO: 441427.0540\n",
      "====> Epoch: 323 Train ELBO: 974290.6244 Val ELBO: 434458.9901\n",
      "====> Epoch: 324 Train ELBO: 983911.6894 Val ELBO: 439961.5568\n",
      "====> Epoch: 325 Train ELBO: 979650.0259 Val ELBO: 444461.3835\n",
      "====> Epoch: 326 Train ELBO: 975775.6547 Val ELBO: 437296.2599\n",
      "====> Epoch: 327 Train ELBO: 967560.3598 Val ELBO: 537385.1818\n",
      "====> Epoch: 328 Train ELBO: 972072.1578 Val ELBO: 547065.6733\n",
      "====> Epoch: 329 Train ELBO: 984127.1528 Val ELBO: 487427.4460\n",
      "====> Epoch: 330 Train ELBO: 980728.0821 Val ELBO: 477928.9730\n",
      "====> Epoch: 331 Train ELBO: 974627.0682 Val ELBO: 499687.8324\n",
      "====> Epoch: 332 Train ELBO: 971531.9918 Val ELBO: 503520.4148\n",
      "====> Epoch: 333 Train ELBO: 972774.8788 Val ELBO: 453090.0320\n",
      "====> Epoch: 334 Train ELBO: 963827.3580 Val ELBO: 540141.2557\n",
      "====> Epoch: 335 Train ELBO: 972501.3598 Val ELBO: 416624.1918\n",
      "====> Epoch: 336 Train ELBO: 981966.0316 Val ELBO: 448506.7884\n",
      "====> Epoch: 337 Train ELBO: 969797.8093 Val ELBO: 504731.8693\n",
      "====> Epoch: 338 Train ELBO: 960593.0928 Val ELBO: 541318.6278\n",
      "====> Epoch: 339 Train ELBO: 936629.2304 Val ELBO: 592575.1477\n",
      "====> Epoch: 340 Train ELBO: 931518.3567 Val ELBO: 650528.3466\n",
      "====> Epoch: 341 Train ELBO: 930641.9482 Val ELBO: 568236.3636\n",
      "====> Epoch: 342 Train ELBO: 940208.7399 Val ELBO: 405800.7589\n",
      "====> Epoch: 343 Train ELBO: 931814.2197 Val ELBO: 469703.6747\n",
      "====> Epoch: 344 Train ELBO: 894800.3782 Val ELBO: 502144.9205\n",
      "====> Epoch: 345 Train ELBO: 909235.3422 Val ELBO: 397852.6570\n",
      "====> Epoch: 346 Train ELBO: 965568.6951 Val ELBO: 553468.1619\n",
      "====> Epoch: 347 Train ELBO: 978094.7216 Val ELBO: 601053.4432\n",
      "====> Epoch: 348 Train ELBO: 979804.3194 Val ELBO: 539945.4432\n",
      "====> Epoch: 349 Train ELBO: 972532.0208 Val ELBO: 460883.7855\n",
      "====> Epoch: 350 Train ELBO: 984178.9918 Val ELBO: 425950.6939\n",
      "====> Epoch: 351 Train ELBO: 938060.9375 Val ELBO: 446775.2585\n",
      "====> Epoch: 352 Train ELBO: 922633.5025 Val ELBO: 480211.8509\n",
      "====> Epoch: 353 Train ELBO: 956642.8510 Val ELBO: 548197.8153\n",
      "====> Epoch: 354 Train ELBO: 958226.1117 Val ELBO: 600572.8011\n",
      "====> Epoch: 355 Train ELBO: 955945.0429 Val ELBO: 543651.1392\n"
     ]
    }
   ],
   "source": [
    "# initialize latent GMM model parameters\n",
    "params = {}\n",
    "device = 0\n",
    "\n",
    "pi_variables = torch.zeros(K, requires_grad = True, device = device)\n",
    "params['pi_c'] = torch.ones(K) / K\n",
    "torch.manual_seed(r)\n",
    "params['mu_c'] = torch.rand((K, Z)) * 2.0 - 1.0\n",
    "params['mu_c'] = params['mu_c']\n",
    "params['logsigmasq_c'] = torch.zeros((K, Z))\n",
    "\n",
    "# initialize neural networks\n",
    "encoder_list = []\n",
    "decoder_list = []\n",
    "trainable_parameters = []\n",
    "trainable_parameters.append(pi_variables)\n",
    "\n",
    "torch.manual_seed(r)\n",
    "encoder = Encoder(encoded_space_dim = latent_dim).to(device)\n",
    "decoder = Decoder(encoded_space_dim = latent_dim).to(device)\n",
    "\n",
    "trainable_parameters += list(encoder.parameters()) + list(decoder.parameters())\n",
    "\n",
    "optimizer = optim.Adam(trainable_parameters, lr=lr)\n",
    "\n",
    "\n",
    "# training\n",
    "train_loss = torch.zeros(num_epochs)\n",
    "rec_loss = torch.zeros(num_epochs)\n",
    "reg_loss = torch.zeros(num_epochs)\n",
    "entr_loss = torch.zeros(num_epochs)\n",
    "\n",
    "valid_loss = torch.zeros(num_epochs)\n",
    "pi_history = torch.zeros((num_epochs, K))\n",
    "min_valid_loss = torch.inf\n",
    "epoch_list = []\n",
    "train_loss_list = []\n",
    "valid_loss_list = []\n",
    "\n",
    "rec_loss_list = []\n",
    "reg_loss_list = []\n",
    "entr_loss_list = []\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    encoder.train()\n",
    "    decoder.train()\n",
    "\n",
    "    train_elbo = 0\n",
    "    rec_elbo = 0\n",
    "    reg_elbo = 0\n",
    "    entr_elbo = 0\n",
    "    gamma_c_epoch = []\n",
    "    params['hist_weights'] = torch.zeros((K, 1)).clone().detach()\n",
    "    params['hist_mu_c'] = torch.zeros((K, Z)).clone().detach()\n",
    "    params['hist_logsigmasq_c'] = torch.zeros((K, Z)).clone().detach()\n",
    "\n",
    "    for (batch_idx, batch_x) in enumerate(train_loader):\n",
    "        \n",
    "        x = batch_x[0].to(device)\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        pi_c = torch.exp(pi_variables) / torch.sum(torch.exp(pi_variables))\n",
    "#         print(f'pi variables: {pi_variables}')\n",
    "#         print(f'pi_c: {pi_c}')\n",
    "\n",
    "        params['pi_c'] = pi_c\n",
    "\n",
    "        mu, logsigmasq = encoder_step(x, encoder, decoder)\n",
    "#         print(f'mu: {mu}')\n",
    "#         print(f'logsigmasq: {logsigmasq}')\n",
    "        sigma = torch.exp(0.5 * logsigmasq)\n",
    "        torch.manual_seed(r)\n",
    "        eps = Normal(0, 1).sample(mu.shape).to(device)\n",
    "        z = mu + eps * sigma\n",
    "#         print(f'z: {z}')\n",
    "\n",
    "        with torch.no_grad():\n",
    "            gamma_c, mu_c, logsigmasq_c = em_step(z, mu, params, update_by_batch=True)\n",
    "            \n",
    "#         print(f'gamma_c: {gamma_c}')\n",
    "#         print(f'mu_c: {mu_c}')\n",
    "#         print(f'logsigmasq_c: {logsigmasq_c}')\n",
    "        params['mu_c'] = mu_c\n",
    "        params['logsigmasq_c'] = logsigmasq_c\n",
    "        gamma_c_epoch.append(gamma_c)\n",
    "\n",
    "        elbo, rec, reg, entr = decoder_step(x, z, encoder, decoder, params, mu, logsigmasq, gamma_c)\n",
    "        \n",
    "        train_elbo += elbo.item()\n",
    "        rec_elbo += rec.item()\n",
    "        reg_elbo += reg.item()\n",
    "        entr_elbo += entr.item()\n",
    "        loss = - elbo\n",
    "        \n",
    "#         print(f'step: {batch_idx} | train_loss: {-train_elbo}')\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "    gamma_c_train_hist.append(torch.vstack(gamma_c_epoch))\n",
    "\n",
    "    encoder.eval()\n",
    "    decoder.eval()\n",
    "\n",
    "    valid_elbo = 0\n",
    "    gamma_c_epoch = []\n",
    "    with torch.no_grad():\n",
    "        for (batch_idx, batch_x) in enumerate(valid_loader):\n",
    "#             print('VALIDATION')\n",
    "            x = batch_x[0].to(device)\n",
    "            mu, logsigmasq = encoder_step(x, encoder, decoder)\n",
    "#             print(f'mu: {mu}')\n",
    "#             print(f'logsigmasq: {logsigmasq}')\n",
    "            sigma = torch.exp(0.5 * logsigmasq)\n",
    "            torch.manual_seed(r)\n",
    "            eps = Normal(0, 1).sample(mu.shape).to(device)\n",
    "            z = mu + eps * sigma\n",
    "#             print(f'z: {z}')\n",
    "            with torch.no_grad():\n",
    "                gamma_c, _, _ = em_step(z, mu, params)\n",
    "#             print(f'gamma_c: {gamma_c}')\n",
    "            gamma_c_epoch.append(gamma_c)\n",
    "            elbo, rec, reg, entr  = decoder_step(x, z, encoder, decoder, params, mu, logsigmasq, gamma_c)\n",
    "            valid_elbo += elbo.item()\n",
    "            \n",
    "    gamma_c_val_hist.append(torch.vstack(gamma_c_epoch))\n",
    "\n",
    "    train_elbo /= len(train_loader.dataset)\n",
    "    valid_elbo /= len(valid_loader.dataset)\n",
    "    # print('====> Epoch: {} Train ELBO: {:.4f} '.format(epoch, train_elbo))\n",
    "    print('====> Epoch: {} Train ELBO: {:.4f} Val ELBO: {:.4f}'.format(epoch, train_elbo, valid_elbo))\n",
    "\n",
    "    train_loss[epoch] = - train_elbo\n",
    "    rec_loss[epoch] = - rec_elbo\n",
    "    reg_loss[epoch] = - reg_elbo\n",
    "    entr_loss[epoch] = - entr_elbo\n",
    "    valid_loss[epoch] = - valid_elbo\n",
    "    pi_history[epoch] = params['pi_c']\n",
    "\n",
    "    if epoch % save_every == 0:\n",
    "        epoch_list.append(epoch)\n",
    "        train_loss_list.append(train_loss[epoch].item())\n",
    "        valid_loss_list.append(valid_loss[epoch].item())\n",
    "        rec_loss_list.append(rec_loss[epoch].item())\n",
    "        reg_loss_list.append(reg_loss[epoch].item())\n",
    "        entr_loss_list.append(entr_loss[epoch].item())\n",
    "        # Plot the first two dimensions of the latents\n",
    "        with torch.no_grad():\n",
    "            means = []\n",
    "            # labels = []\n",
    "            for batch_x in train_loader:\n",
    "                x = batch_x[0].to(device)\n",
    "                mean, _ = encoder_step(x, encoder, decoder)\n",
    "                means.append(mean)\n",
    "                \n",
    "                # labels.append(batch_label)\n",
    "\n",
    "        means = torch.vstack(means).cpu()\n",
    "        # labels = torch.hstack(labels)\n",
    "        means_hist.append(means)\n",
    "        mu_c_hist.append(params['mu_c'].cpu())\n",
    "        logsigmasq_c_hist.append(params['logsigmasq_c'].cpu())\n",
    "        fig, ax = plt.subplots(figsize=(6, 5))\n",
    "        for i in range(K):\n",
    "            means_i = means[train_labels == i]\n",
    "            ax.scatter(means_i[:, 0], means_i[:, 1], alpha=0.25, label=str(i))\n",
    "        for i in range(K):\n",
    "            ax.plot(params['mu_c'].cpu()[i, 0], params['mu_c'].cpu()[i, 1], 'x', markersize=12) #, label='$\\mu$' + str(i + 1))\n",
    "        \n",
    "        ax.set_xlabel('$z_1$')\n",
    "        ax.set_ylabel('$z_2$')\n",
    "        ax.legend(loc='center left', bbox_to_anchor=(1, 0.5))\n",
    "        fig.tight_layout()\n",
    "        plt.close()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a6bd15b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "path = f'./results/{sim_name}/'\n",
    "#plt.plot(train_e)\n",
    "if os.path.exists(path):\n",
    "    shutil.rmtree(path)\n",
    "os.makedirs(path)\n",
    "\n",
    "# Compute the mean of the latents given the data\n",
    "encoder.eval()\n",
    "\n",
    "with torch.no_grad():\n",
    "    means = []\n",
    "    # labels = []\n",
    "    for batch_x in train_loader:\n",
    "        x = batch_x[0].to(device)\n",
    "        mean, _ = encoder_step(x, encoder, decoder)\n",
    "        means.append(mean)\n",
    "        # labels.append(batch_label)\n",
    "\n",
    "means = torch.vstack(means)\n",
    "# labels = torch.hstack(labels)\n",
    "\n",
    "with torch.no_grad():\n",
    "    gamma_c, mu_c, logsigmasq_c = em_step(means, means, params, update_by_batch=False)\n",
    "\n",
    "my_datamodule.setup('test')\n",
    "test_loader = my_datamodule.test_dataloader()\n",
    "with torch.no_grad():\n",
    "    means_test = []\n",
    "    for batch_x in test_loader:\n",
    "        x = batch_x[0].to(device)\n",
    "        mean, _ = encoder_step(x, encoder, decoder)\n",
    "        means_test.append(mean)\n",
    "        # labels.append(batch_label)\n",
    "means_test = torch.cat(means_test)\n",
    "\n",
    "my_datamodule.setup('outliers')\n",
    "outliers_loader = my_datamodule.outliers_dataloader()\n",
    "with torch.no_grad():\n",
    "    means_outliers = []\n",
    "    for batch_x in outliers_loader:\n",
    "        x = batch_x[0].to(device)\n",
    "        mean, _ = encoder_step(x, encoder, decoder)\n",
    "        means_outliers.append(mean)\n",
    "        # labels.append(batch_label)\n",
    "means_outliers = torch.cat(means_outliers)\n",
    "\n",
    "# Plot the first two dimensions of the latents\n",
    "# plot_params()\n",
    "fig, ax = plt.subplots(figsize=(6, 5))\n",
    "cluster_means = torch.zeros((K, Z))\n",
    "for i in range(K):\n",
    "    means_i = means[train_labels == i].cpu()\n",
    "    ax.scatter(means_i[:, 0], means_i[:, 1], alpha=0.25, label=str(i))\n",
    "    cluster_means[i] = torch.mean(means_i, dim=0)\n",
    "for i in range(K):\n",
    "    ax.plot(params['mu_c'].cpu()[i, 0], params['mu_c'].cpu()[i, 1], 'x', markersize=12, color = 'k')  # , label='$\\mu$' + str(i + 1))\n",
    "\n",
    "ax.scatter(means_test[:, 0].cpu(), means_test[:, 1].cpu(), alpha=1, color = 'g')\n",
    "ax.scatter(means_outliers[:, 0].cpu(), means_outliers[:, 1].cpu(), alpha=1, color = 'r')\n",
    "ax.set_xlabel('$z_1$ mean')\n",
    "ax.set_ylabel('$z_2$ mean')\n",
    "ax.legend(loc='center left', bbox_to_anchor=(1, 0.5))\n",
    "fig.tight_layout()\n",
    "plt.savefig(path +'latent.png', dpi=600)\n",
    "\n",
    "\n",
    "# Plot training loss vs. epoch number\n",
    "plt.figure(figsize=(4.5, 4))\n",
    "const = min(train_loss)\n",
    "train_loss_adjusted = train_loss - const + 10.\n",
    "# val_loss_adjusted = val_loss - const + 10.\n",
    "plt.semilogy(train_loss_adjusted, label='train')\n",
    "# plt.semilogy(val_loss_adjusted, label='val')\n",
    "plt.xlabel(\"number of epochs\")\n",
    "# plt.legend()\n",
    "plt.tight_layout()\n",
    "plt.savefig(path +'train_loss.png', dpi=600)\n",
    "plt.close()\n",
    "\n",
    "# Plot the training and validation loss vs. epoch number\n",
    "plt.figure(figsize=(4.5, 4))\n",
    "const = min(min(train_loss), min(valid_loss))\n",
    "train_loss_adjusted = train_loss - const + 10.\n",
    "valid_loss_adjusted = valid_loss - const + 10.\n",
    "plt.semilogy(train_loss_adjusted, label='train')\n",
    "plt.semilogy(valid_loss_adjusted, label='val')\n",
    "plt.xlabel(\"number of epochs\")\n",
    "plt.legend()\n",
    "plt.tight_layout()\n",
    "plt.savefig(path +'loss.png', dpi=600)\n",
    "plt.close()\n",
    "\n",
    "# Plot the history of pi\n",
    "plt.figure(figsize=(4.5, 4))\n",
    "for i in range(K):\n",
    "    plt.plot(pi_history[:, i].detach().numpy(), label='$\\pi$' + str(i+1))\n",
    "plt.xlabel(\"number of epochs\")\n",
    "plt.legend()\n",
    "plt.tight_layout()\n",
    "plt.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8863f884",
   "metadata": {},
   "outputs": [],
   "source": [
    "# plt.plot(train_loss_list)\n",
    "# plt.plot(val_loss_list)\n",
    "plt.plot(rec_loss_list)\n",
    "plt.plot(reg_loss_list)\n",
    "plt.plot(entr_loss_list)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d8ee7942",
   "metadata": {},
   "outputs": [],
   "source": [
    "results_dic = {}\n",
    "results_dic['epochs'] = epoch_list\n",
    "results_dic['train_loss_epoch'] = train_loss_list\n",
    "results_dic['valid_loss_epoch'] = valid_loss_list\n",
    "results_dic['reconstruction_loss_epoch'] = rec_loss_list\n",
    "results_dic['regularization_loss_epoch'] = reg_loss_list\n",
    "results_dic['entropy_loss_epoch'] = entr_loss_list\n",
    "results_dic['mu_c'] = [mu_c.cpu().numpy().tolist() for mu in mu_c_hist]\n",
    "results_dic['logsigmasq_c'] = [logsig.cpu().numpy().tolist() for logsig in logsigmasq_c_hist]\n",
    "results_dic['means'] = [mu.cpu().numpy().tolist() for mu in means_hist]\n",
    "results_dic['pi'] = [pi.cpu().detach().numpy().tolist() for pi in pi_history]\n",
    "results_dic['gamma_c_train'] = [gamma.cpu().detach().numpy().tolist() for gamma in gamma_c_train_hist]\n",
    "results_dic['train_labels'] = train_labels.tolist()\n",
    "results_dic['gamma_c_val'] = [gamma.cpu().detach().numpy().tolist() for gamma in gamma_c_val_hist]\n",
    "results_dic['val_labels'] = valid_labels.tolist()\n",
    "\n",
    "with open(path + f'GMVAE_rocks_K1_Z4.json', 'w') as outfile:\n",
    "    json.dump(results_dic, outfile)\n",
    "    \n",
    "%xdel results_dic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "79205b44",
   "metadata": {},
   "outputs": [],
   "source": [
    "xmax = np.max(means_hist[0].numpy()[:,0])\n",
    "ymax = np.max(means_hist[0].numpy()[:,1])\n",
    "xmin = np.min(means_hist[0].numpy()[:,0])\n",
    "ymin = np.min(means_hist[0].numpy()[:,1])\n",
    "max_epoch = np.max(np.array(epoch_list))\n",
    "\n",
    "for i in range(len(means_hist)):\n",
    "    means = means_hist[i].numpy()\n",
    "    if np.max(means[:,0]) > xmax:\n",
    "        xmax = np.max(means[:,0])\n",
    "    if np.max(means[:,1]) > ymax:\n",
    "        ymax = np.max(means[:,1])\n",
    "    if np.min(means[:,0]) < xmin:\n",
    "        xmin = np.min(means[:,0])\n",
    "    if np.min(means[:,1]) < ymin:\n",
    "        ymin = np.min(means[:,1])\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ab16348",
   "metadata": {},
   "outputs": [],
   "source": [
    "Writer = animation.writers['ffmpeg']\n",
    "writer = Writer(fps=20, metadata=dict(artist='Me'))\n",
    "\n",
    "fig, axs = plt.subplots(1,2,figsize=(13, 5))\n",
    "train_loss_list = np.array(train_loss_list)\n",
    "valid_loss_list = np.array(valid_loss_list)\n",
    "rec_loss_list = np.array(rec_loss_list)\n",
    "reg_loss_list = np.array(reg_loss_list)\n",
    "entr_loss_list = np.array(entr_loss_list)\n",
    "epoch_list = np.array(epoch_list)\n",
    "\n",
    "fig.suptitle(f'GMVAE | latent dim. = {latent_dim}, # clusters = {K}', fontsize=16)\n",
    "def animate(j):\n",
    "    print(j)\n",
    "    #Axis 1\n",
    "    means = means_hist[j]\n",
    "    mu_c = mu_c_hist[j]\n",
    "    sigmaq_c = np.exp(logsigmasq_c_hist[j].numpy())\n",
    "    axs[0].clear()\n",
    "    axs[1].clear()\n",
    "    for i in range(2):\n",
    "        means_i = means[train_labels == i]\n",
    "        axs[0].scatter(means_i[:, 0], means_i[:, 1], alpha=0.25, label=str(i))\n",
    "    for i in range(K):\n",
    "        axs[0].plot(mu_c[i, 0], mu_c[i, 1], 'x', markersize=12) #, label='$\\mu$' + str(i + 1))\n",
    "        c = axs[0].get_lines()[0].get_color()\n",
    "        ellipse = Ellipse((mu_c[i, 0], mu_c[i, 1]),\n",
    "        width=2*3*sigmaq_c[i, 0],\n",
    "        height=2*3*sigmaq_c[i, 1],\n",
    "        facecolor='None',\n",
    "        edgecolor = c,\n",
    "        linestyle = '--')\n",
    "        axs[0].add_patch(ellipse)\n",
    "        \n",
    "    axs[0].set_xlabel('$z_1$')\n",
    "    axs[0].set_ylabel('$z_2$')\n",
    "    axs[0].set_xlim([xmin, xmax])\n",
    "    axs[0].set_ylim([ymin, ymax])\n",
    "    axs[0].legend(loc='center left', bbox_to_anchor=(1, 0.5))\n",
    "\n",
    "    #Axis 2\n",
    "    axs[1].plot(epoch_list[0:j], train_loss_list[0:j] / train_loss[0],'k')\n",
    "    axs[1].plot(epoch_list[0:j], valid_loss_list[0:j] / valid_loss[0],'m')\n",
    "    axs[1].plot(epoch_list[0:j], rec_loss_list[0:j] / rec_loss[0],'r--')\n",
    "    axs[1].plot(epoch_list[0:j], reg_loss_list[0:j] / reg_loss[0],'g--')\n",
    "    axs[1].plot(epoch_list[0:j], entr_loss_list[0:j] / entr_loss[0],'b--')\n",
    "    axs[1].set_xlabel('$epoch #$')\n",
    "    axs[1].set_ylabel('$loss$')\n",
    "    axs[1].set_xlim([0, max_epoch])\n",
    "#     ax2.set_ylim([ymin, ymax])\n",
    "    axs[1].legend(['train loss', 'val loss', 'rec. loss', 'reg. loss', 'entropy loss'], loc='center left', bbox_to_anchor=(1, 0.5))\n",
    "    fig.tight_layout()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea393021",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "ani = matplotlib.animation.FuncAnimation(fig, animate, frames=len(epoch_list), repeat=False)\n",
    "ani.save(path + 'training_video.mp4', writer=writer, dpi=600)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae638610",
   "metadata": {},
   "outputs": [],
   "source": [
    "path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "87c1ddaa",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
